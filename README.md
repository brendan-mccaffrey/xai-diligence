# xAI
----
A diligence report on Elon Musk's newest venture.

## Overview

- Announced July 12, 2023
- Main Goal: Build a good AGI with the purpose of trying to understand the universe
- Regulation: They say it's important.

Ethics: 
- safest way to build an AI is to make one maximally curious and truth-seeking, with acknolwedgement of error.
- very dangerous to create a politically correct AGI, essentially teching the AGI to lie
- AGI Threshold: Can't call something AGI until it's solved at least one fundamental, important problem.

## Team

**Collective Resume**

Experience: Deepmind, OpenAI, Google Research, Microsoft Research, Tesla, UToronto.

Contributions: 
- Adam Optimizer
- Batch Normalization
- Layer Normalization
- discovery of [adversarial examples](https://arxiv.org/abs/1312.6199)
- TransformerXL
- Autoformalization
- Memorizing transfer
- batch size scaling
- μTransfer

Involed in
- Alphastar
- AlphaCode
- Inception
- Minerva
- GPT 3.5
- GPT 4

## Individuals

Elon Musk

#### Dan Hendrycks, Advisor
- currently leads the [Center for AI Safety](https://www.safe.ai/)

#### Igor Babuschkin

- worked at the large hadron collider, very interested in the nature of the universe
- became interested in AI and deep learning
- worked at deepmind from 2017 to 2020, worked on Alphastar 
- joined OpenAI after Deepmind, worked on GPT 3.5
- was arrested for domestic abuse in April, release on $10k bond
- has very successful research, including Grandmaster level in Starcraft II, 2019, 3.2k citations; LLMs trained on code, many more
- During Babuschkin’s second term at the Deepmind, he was a Senior Staff Research Engineer from April 2022 to February 2023.

Babuschkin spent nearly two years at OpenAI until March of last year, when he rejoined DeepMind as a senior staff research engineer.

In February of this year, he ended his stint at DeepMind, which was right around the time that it was reported that Musk sought to enlist Babuschkin in his quest to develop a rival chatbot to “woke” ChatGPT.

#### Manuel Kroiss
- Was at DeepMind, worked on reinforcement learning including Alphastar
- excited by AI for a long time

#### Christian Szegedy
- Passionate about making AI as good at math as any human, AI needs deep resoning
- Senior Research at Google, focusing on AI for math
- discovered qadversarial examples
- invented BatchNorm
- pivotal computer vision research
- incredibly successful research, 250k citations
- research from 2013

#### Tony Wu
- [Backgorund](https://yuhuaiwu.github.io/): Autoformalization, [Minerva](https://arxiv.org/abs/2206.14858), Memorizing transformer, STaR, AlphaStar, LIME
- Interesting progress with AI for math (Minerva)
- OpenAI Research intern 2017
- DeepMind Intern 2018-2019
- Previously at Google N2Formal led by Christian Szegedy, 2021-2023
- [Minerva: Solving Quantitative Reasoning Problems with Language Models](https://ai.googleblog.com/2022/06/minerva-solving-quantitative-reasoning.html)
- Bootstrapping Reasoning with Reasoning
- really next level stuff

#### Jimmy Ba
- Sloan fellow, teaches at University of Toronto
- does neural nets, explored many aspects of deep learning
- derived building blocks for modern transformers
- long term ambition, aligns with xAI team, general purpose problem solving machines
- completed PhD under the supervision of Geoffrey Hinton. 
- recipient of the Facebook Graduate Fellowship 2016 in machine learning
- extremely legit

#### Toby Pohlen
- from germany, began coding very young
- background in computer vision
- joined DeepMind 6 yrs ago, imitation learning and reinforcement learning
- believes accessible and useful AI is important
- [A data-driven approach for learning to control computers](https://arxiv.org/abs/2202.08137)

#### Kyle Kosic
- distributed systems engineer
- math and applied physics
- OpenAI hbc problems, specifically GPT 4
- excited about xAI bc biggest danger is monopolization

#### Greg Yang
- math and science of deep learning
- undergrad harvard 10yr ago, good at math (morgan prize HM)
- took time off after 2yr, became a DJ & producer, explored fundamental questions
- math is underlying language of reality, AI must know math
- can hold a convo with any mathematician in the world
- worked at microsoft research, made fundamental contributions to large scale neural networks
- tensor programs, μTransfer
- quantum physics and general relativity
- tensor program: a composition of matrix multiplication and coordinatewise nonlinearities.

#### Guodong Zhang
- Focus on training, tuning, and aligning large language models good.
- worked at deepmind, worked on Gemini - [expected to surpass GPT-4](https://www.wired.com/story/google-deepmind-demis-hassabis-chatgpt/)
- U Toronto PhD
- a ton of publications

#### Zihang Dai
- Carnegie PhD
- utilize data, transformer architecture
- Google AI brain team
- transformer-xl
- autoregressive pretraining
- confitional focused neural questioning

#### Ross Nordeen
- large distributed systems
- palantir, tesla, twitter
- power cooling, manufacturing, effectively a generalist
- former technical program manager at Tesla's supercomputing and machine learning division


## Notes

- It's important to the team that their progress is shared with the world, so expect them to release apps and tools from the beginning.
- They believe keeping a small team and high compute is effective for the progress of xAI.
- xAI is competition to OpenAI and Google BARD
- Musk said every organization doing AI has illegally used Twitter’s data for training. Limits had to be put on Twitter because they were being scraped like crazy. Multiple entities were trying to scrape every tweet ever made in a span of days. xAI will use tweets as well for training.
- Musk said there is a significant danger in training AI to be politically correct or training it not to say what it thinks is true, so at xAI they will let the AI say what it believes to be true, and Musk believes it will result in some criticism (I'm bullish this take)
- Musk believes we will have a voltage transformer shortage in a year and electricity shortage in 2 years.
- xAI will work with Tesla in multiple ways and it will be of mutual benefit. Tesla’s self-driving capabilities will be enhanced because of xAI. Tesla will be hlepful (on the silicon front)
- OpenAI LLMs are getting worse, not better, over time 
    - [paper](https://arxiv.org/pdf/2307.09009.pdf)
    - [tweet](https://twitter.com/svpino/status/1681614284613099520?s=42&t=mKfMvA9kDRr3LdnI-MLdig)

**Elon Synergies**
- Twitter, xAI is only corp with legal access to Twitter data corpus
- Silicon: Tesla "will have significant advantage in energy-efficient inference", DOJO will be able to run LLMs efficiently. xAI will also help the self-driving endeavor.
- SpaceX: will put "fundamental physics" insights to work, essentially a B2B client of xAI


## Analyses

1. By Jim Fan Senior Nvidia AI Scientist

*I see a few unique strengths in Elon's ecosystem:*

*▸ Lots of multimodal data on Twitter: dialogue text, images, and a growing collection of long videos. http://X.ai is the only AI company that has direct & legal access to such an enormous & daily-expanding corpus.
▸ Tesla FSD team has years of experience in building huge training clusters, such as Dojo. Not to mention the tons of high-quality Tesla fleet data for machine perception.
▸ Tesla Bot would be the physical embodiment of the http://X.ai brain, if they ever connect the two.*

*It's also got an all-star founding team. I'm really impressed by the talent density - read too many papers from them to count:*

*- Jimmy Ba: Adam (default optimizer for modern NNs), Layer Normalization (critical building block of Transformer).
- Igor Babuschkin: AlphaStar (DeepMind's StarCraft player that beats human champions).
- Christian Szegedy: BatchNorm (key component of ResNet that made training stable).
- Yuhuai "Tony" Wu: AlphaStar, OpenAI Baselines (widely adopted RL library), PaLM-2
- Greg Yang: deep learning theory
- Zihang Dai: Transformer-XL (adding recurrence to extend beyond a fixed context window), XLNet*
